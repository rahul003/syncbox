source base
{
	type			= mysql
	sql_host		= {{ DATABASE_HOST }}
	sql_user		= {{ DATABASE_USER }}
	sql_pass		= {{ DATABASE_PASSWORD }}
	sql_db			= {{ DATABASE_NAME }}
	sql_port		= {{ DATABASE_PORT }}

	sql_query_pre	= SET NAMES utf8
	sql_query_post	=
}

source cities : base
{
	sql_query			= \
		SELECT id, state_id, name, aliases FROM cities WHERE visible = 1
	sql_query_info		= SELECT * FROM `cities` WHERE id = $id

	sql_attr_uint		= state_id
}
index cities
{
	docinfo			= extern
	morphology		= stem_en
	stopwords		=
	min_word_len	= 2
	charset_type	= sbcs
	min_prefix_len	= 0
	min_infix_len	= 0
	enable_star	 	= 1
	source			= cities
	path			= {{ relative_path('data', 'cities') }}
	wordforms		= {{ relative_path('data', 'wordforms.txt') }}
}

indexer
{
	# memory limit, in bytes, kiloytes (16384K) or megabytes (256M)
	# optional, default is 32M, max is 2047M, recommended is 256M to 1024M
	mem_limit		= 256M

	# maximum IO calls per second (for I/O throttling)
	# optional, default is 0 (unlimited)
	#
	# max_iops		= 40

	# maximum IO call size, bytes (for I/O throttling)
	# optional, default is 0 (unlimited)
	#
	max_iosize		= 524288
}

searchd
{
	listen			= {{ SPHINX_HOST }}:{{ SPHINX_PORT }}

	# log file, searchd run info is logged here
	# optional, default is 'searchd.log'
	log				= {{ relative_path('log', 'searchd.log') }}

	# query log file, all search queries are logged here
	# optional, default is empty (do not log queries)
	query_log		= {{ relative_path('log', 'query.log') }}

	# client read timeout, seconds
	# optional, default is 5
	read_timeout	= 5

	# maximum amount of children to fork (concurrent searches to run)
	# optional, default is 0 (unlimited)
	max_children	= 30

	# PID file, searchd process ID file name
	# mandatory
	pid_file		= {{ relative_path('run', 'searchd.pid') }}

	# max amount of matches the daemon ever keeps in RAM, per-index
	# WARNING, THERE'S ALSO PER-QUERY LIMIT, SEE SetLimits() API CALL
	# default is 1000 (just like Google)
	max_matches		= 1000

	# seamless rotate, prevents rotate stalls if precaching huge datasets
	# optional, default is 1
	seamless_rotate	= 1

	# whether to forcibly preopen all indexes on startup
	# optional, default is 0 (do not preopen)
	preopen_indexes	= 0

	# whether to unlink .old index copies on succesful rotation.
	# optional, default is 1 (do unlink)
	unlink_old		= 1
}